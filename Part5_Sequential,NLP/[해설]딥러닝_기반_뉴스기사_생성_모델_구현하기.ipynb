{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"[해설]딥러닝_기반_뉴스기사_생성_모델_구현하기.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U6iOz3KxXkGK"},"source":["# 주제 : 뉴스기사 생성 모델 구현하기"]},{"cell_type":"markdown","metadata":{"id":"jioUvCUIXnhe"},"source":["이번 튜토리얼에서는 LSTM layer를 활용하여 가짜 뉴스기사 생성기를 만들어 보도록 하겠습니다. 튜토리얼을 진행하면서 **한글 자연어 전처리, 학습을 위한 데이터셋 구축, 그리고 LSTM 텍스트 생성기 모델** 완성을 학습하실 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"lEsQyK6GErgR"},"source":["## Step 1. 데이터 불러오기 및 전처리"]},{"cell_type":"markdown","metadata":{"id":"yeYlgTdfEzV3"},"source":["### 문제 01. 필요한 모듈 import"]},{"cell_type":"code","metadata":{"id":"okTryvaYEfVZ"},"source":["import tensorflow as tf\n","import numpy as np\n","import time\n","import pandas as pd\n","import os\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5KIbKYuU1B1k"},"source":["### 문제 02. 데이터 불러오기"]},{"cell_type":"markdown","metadata":{"id":"jVq4fqx-1B1l"},"source":["- 한글 뉴스기사 데이터셋을 받습니다.\n","- 한글 뉴스기사 200개의 데이터셋입니다.\n","- IT 관련 기사 데이터셋입니다. IT 관련 뉴스기사를 학습 시키므로, 향후 예측시 IT 관련된 뉴스기사가 생성됩니다."]},{"cell_type":"code","metadata":{"id":"XHPd6ZDw1B1l"},"source":["df = pd.read_csv('https://bit.ly/3n7iHQX')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M_BEtxR91B1l"},"source":["- 전처리를 진행합니다.\n","- re 모듈을 사용하며 regular expression 문법을 사용하여, 한글, 영어, 숫자를 제외한 모든 문자는 제거합니다.\n","- 문장의 끝에는 '#'를 추가하여, 신문 기사의 끝이라는 표기를 해줍니다."]},{"cell_type":"code","metadata":{"id":"obn2pKB01B1m"},"source":["def clean_sentence(sentence):\n","    # 한글, 영어, 숫자를 제외한 모든 문자는 제거합니다.\n","    sentence = re.sub(r'[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣 ]',r'', sentence)\n","    # 문장의 끝을 표기합니다.\n","    sentence += ' #'\n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCLu-i8n1B1m","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"fa09fcdd-def9-48ba-a121-67b1f99c7727"},"source":["clean_sentence('abcef가나다^^$%@12시 땡^^!??')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'abcef가나다12시 땡 #'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"SytnQm_T1B1n"},"source":["df['text'] = df['text'].apply(clean_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IXvn4fH61B1n","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"2848065a-f712-4b9c-acc5-9545691a3fd5"},"source":["df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>갤럭시S9 20만 원대 아이폰6S 0원 모비톡 가정의 달 이벤트갤럭시노트8 갤럭시S...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LG 그램 100만대 판매기념 한정판 나왔다LG전자가 그램 노트북 누적판매 100만...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>이게 정말 LG폰이에요G7 씽큐 기분 좋은 스타트20일 서울 신촌역 앞 한 휴대폰 ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>애플 10억불vs 삼성 2800만불배상액 종지부 눈앞삼성애플 둥근모서리 디자인특허침...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>삼성전자 5G 국제 표준 주도한다삼성전자가 5세대5G 이동통신 1차 표준 완성을 위...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text\n","0  갤럭시S9 20만 원대 아이폰6S 0원 모비톡 가정의 달 이벤트갤럭시노트8 갤럭시S...\n","1  LG 그램 100만대 판매기념 한정판 나왔다LG전자가 그램 노트북 누적판매 100만...\n","2  이게 정말 LG폰이에요G7 씽큐 기분 좋은 스타트20일 서울 신촌역 앞 한 휴대폰 ...\n","3  애플 10억불vs 삼성 2800만불배상액 종지부 눈앞삼성애플 둥근모서리 디자인특허침...\n","4  삼성전자 5G 국제 표준 주도한다삼성전자가 5세대5G 이동통신 1차 표준 완성을 위..."]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"j6MY4qCCEfVm"},"source":["### 문제 03. 데이터 프레임에서 text만 병합하기"]},{"cell_type":"markdown","metadata":{"id":"SyzhalHI1B1o"},"source":["`text` 변수에 데이터프레임의 담긴 모든 기사를 join하여 병합합니다."]},{"cell_type":"code","metadata":{"id":"dP8-a8VmEfVn"},"source":["text = ' '.join(df['text'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6HA5IDD01B1o"},"source":["총 문장의 길이는 다음과 같습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jCZwAMbwEfVn","outputId":"d5d96df7-fcbd-4385-edec-7af87680a79b"},"source":["# 총 문장의 길이\n","len(text)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["222853"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"NwGCGTy41B1p"},"source":["문장의 500 글자만 출력해 봅니다. 전처리가 완료된 문장이 출려됨을 확인할 수 있습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1pXJ8x8EfVp","outputId":"e985dfe9-b441-48e9-a337-f539b8377cf2"},"source":["print(text[:500])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["갤럭시S9 20만 원대 아이폰6S 0원 모비톡 가정의 달 이벤트갤럭시노트8 갤럭시S9 갤럭시S8 갤럭시S7 갤럭시S7엣지 아이폰6S 아이폰X 아이폰8 G7 G6 V30 등 다양한 휴대폰 정보가 가득한 스마트폰 공동구매 및 거래 어플 모비톡의 가정의 달 이벤트가 화제다모비톡 단독으로 진행되는 5월 가정의 달 이벤트에 이용자들의 폭발적인 반응이 나타나고 있다 고가의 인기 스마트폰을 파격가에 판매한다는 사실에 각종 커뮤니티와 카페를 중심으로 화제를 모으고 있는 것 특히 갤럭시S9를 20만 원대 아이폰6S는 0원 할부원금을 앞세워 안드로이드와iOS인기 기종을 중심으로 큰 폭의 할인을 펼치는게 주된 요인으로 꼽힌다 모비톡 관계자에 따르면 고마운 사람들에게 감사한 마음을 담아 선물할 기회가 많은 5월 가정의 달을 맞아 공격적인 마케팅을 진행하고 있다며 독보적인 통신비 절약 어플로서 앞으로도 최선을 다하겠다고 밝혔다이 밖에도 모비톡은 갤럭시노트8 V30 구매 시 닌텐도 스위치를 증정한다 스마트폰\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hGO7N_2-EfVp"},"source":["### 문제 04. 텍스트 기본 전처리 (preprocessing)"]},{"cell_type":"markdown","metadata":{"id":"G6TT6-Rf1B1q"},"source":["단어 사전을 만듭니다. 먼저, 중복되는 모든 글자를 제외하기 위하여 **set를 활용**합니다."]},{"cell_type":"code","metadata":{"id":"tMzJc2WnEfVq"},"source":["vocab = sorted(set(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ahEXvOtEfVq","outputId":"34547729-ce97-45c1-fd89-5db6b4fdfcb2"},"source":["vocab[:20]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[' ',\n"," '#',\n"," '0',\n"," '1',\n"," '2',\n"," '3',\n"," '4',\n"," '5',\n"," '6',\n"," '7',\n"," '8',\n"," '9',\n"," 'A',\n"," 'B',\n"," 'C',\n"," 'D',\n"," 'E',\n"," 'F',\n"," 'G',\n"," 'H']"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"b55h8e2z1B1q"},"source":["고유 글자의 숫자를 확인합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPcVmJDTEfVr","outputId":"a6bc76e0-11e2-45c2-b3df-22e7436ae2f5"},"source":["# 고유 글자의 숫자 확인\n","len(vocab)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1172"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"XS_8rjbg1B1r"},"source":["'?'라는 글자는 `vocab` 변수에 없는 글자임을 확인할 수 있습니다. 전처리 단계에서 제거했기 때문에 당연히 없습니다."]},{"cell_type":"code","metadata":{"id":"UFblnqXl1B1r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ab0fef58-3519-4e6b-8236-42341b59877d"},"source":["'?' in vocab"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"VLg7pjQi1B1r"},"source":["**'?'** 글자를 추가해 주는데, 추후 사용자의 입력이 없는 글자일 때는 ?로 입력하기 위함입니다."]},{"cell_type":"code","metadata":{"id":"fmFA3igN1B1s"},"source":["vocab.append('?')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WGVhTYP2EfVr"},"source":["### 문제 05. 데이터 형태 변환하기"]},{"cell_type":"markdown","metadata":{"id":"0wVsb0Ob1B1s"},"source":["`char2idx`는 글자를 index로 변환하는 역할이고, `idx2char`는 index를 글자로 역변환하는 목적입니다."]},{"cell_type":"code","metadata":{"id":"nFIzFLfAEfVs"},"source":["#글자 -> index로 변환\n","char2idx = {u: i for i, u in enumerate(vocab)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3Dlmyk8EfVs"},"source":["#index -> 글자로 변환\n","idx2char = np.array(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YTf9OO4vEfVs"},"source":["## STEP 2. 단어 사전 만들기"]},{"cell_type":"markdown","metadata":{"id":"1TOzDoEtEfVt"},"source":["### 문제 06. for문을 사용해 문서를 연속된 수치형 값들로 치환합니다."]},{"cell_type":"code","metadata":{"id":"a8_Bp_U2EfVt"},"source":["text_as_int = np.array([char2idx[c] for c in text])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzNMj8EKEfVt","outputId":"5a1770f9-5a77-48e6-b097-8ec2251f48aa"},"source":["text_as_int"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 76, 394, 666, ..., 266,   0,   1])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuVELDFxEfVt","outputId":"83c6a051-2ef5-4114-df59-c498f700bdfe"},"source":["len(text_as_int)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["222853"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"aVfWkUnHEfVu"},"source":["### 문제 07. 변환된 부분을 확인합니다. (처음 5개)"]},{"cell_type":"markdown","metadata":{"id":"OWXtqehA1B1u"},"source":["**원문의 출력**은 다음과 같습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"RPXOEWvGEfVu","outputId":"7dc5cb77-895e-41d5-a0b7-6737018ab4fc"},"source":["# 원문\n","text[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'갤럭시S9'"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"Ct5ijCC-1B1u"},"source":["**sequence로 변환**된 출력은 다음과 같습니다. 한글자씩 변환된 것을 볼 수 있습니다."]},{"cell_type":"code","metadata":{"id":"XR0foxBa1B1v","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cafc4e49-1021-4060-e407-5a170ebeaa35"},"source":["char2idx['갤'], char2idx['럭'], char2idx['시'], char2idx['S'], char2idx['9']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(76, 394, 666, 30, 11)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EC7RBq9aEfVu","outputId":"c66c114d-f2b1-48b9-f9ae-1eff70563bf5"},"source":["# 변환된 sequence\n","text_as_int[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 76, 394, 666,  30,  11])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"DmM0XnCKYpen"},"source":["### 문제 08. 각각의 단어사전으로 출력합니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uY5B2EKyEfVu","outputId":"16b0b7ce-dd29-417a-fa27-a681b89f6547"},"source":["char2idx[' '], char2idx['회'], char2idx['사'], char2idx['#'], char2idx['?'], "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 1144, 599, 1, 1172)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"DG422zZJajrQ"},"source":["## Step 3. 데이터셋 생성 및 EDA"]},{"cell_type":"markdown","metadata":{"id":"r4RB9pRmEfVv"},"source":["### 문제 09. X, Y 데이터셋 생성하기"]},{"cell_type":"code","metadata":{"id":"nxLyqzaLEfVv"},"source":["# 아래 코드는 그대로 실행해주세요.\n","# 단일 입력에 대해 원하는 문장의 최대 길이를 지정합니다.\n","window_size = 100\n","shuffle_buffer = 1000\n","batch_size=128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S35twbV6EfVv"},"source":["# 데이터셋을 만드는 함수를 구현해봅니다.\n","def windowed_dataset(series, window_size, shuffle_buffer, batch_size):\n","    series = tf.expand_dims(series, -1)\n","    ds = tf.data.Dataset.from_tensor_slices(series)\n","    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n","    ds = ds.flat_map(lambda x: x.batch(window_size + 1))\n","    ds = ds.shuffle(shuffle_buffer)\n","    ds = ds.map(lambda x: (x[:-1], x[1:]))\n","    return ds.batch(batch_size).prefetch(1).repeat()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XQGsUYY-EfVv"},"source":["train_data = windowed_dataset(np.array(text_as_int), window_size, shuffle_buffer, batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pBFDZ0dmY7Qg"},"source":["### 문제 10. 어휘 사전의 크기를 간단히 살펴봅니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_nx-mWnEfVw","outputId":"f305e627-01d6-4312-bc5e-e58b5c85bf3b"},"source":["# 문자로 된 어휘 사전의 크기\n","vocab_size = len(vocab)\n","vocab_size"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1173"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"-D6SKQowaojm"},"source":["## Step 4.Sequential 모델 구현하기"]},{"cell_type":"markdown","metadata":{"id":"0vPMeRLGZCQI"},"source":["### 문제 11. keras를 활용해 Sequential 모델을 구현합니다."]},{"cell_type":"markdown","metadata":{"id":"rYubFDhe1B1x"},"source":["hyperparameter를 다음과 같이 설정합니다. 데이터셋에 따라서 언제든 변경하면서 더 좋은 성능을 내는 hyperparameter 값을 찾을 수 있습니다."]},{"cell_type":"code","metadata":{"id":"qgQYOBoeEfVw"},"source":["# 아래 코드는 그대로 실행해주세요.\n","# 임베딩 차원\n","embedding_dim = 256\n","\n","# RNN 유닛(unit) 개수\n","rnn_units = 1024"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2akdn8nVEfVw","scrolled":true},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=window_size),\n","    tf.keras.layers.LSTM(rnn_units,\n","                         return_sequences=True,\n","                         recurrent_initializer='glorot_uniform'),\n","    tf.keras.layers.Dense(vocab_size, activation='softmax')\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mpkPQjxEfVw","outputId":"13f03c32-e063-489a-d08f-c3842b32722b"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 100, 256)          300288    \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 100, 1024)         5246976   \n","_________________________________________________________________\n","dense (Dense)                (None, 100, 1173)         1202325   \n","=================================================================\n","Total params: 6,749,589\n","Trainable params: 6,749,589\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Pds1usgEfVx"},"source":["### 문제 12. 모델을 저장할 Checkpoint를 생성합니다."]},{"cell_type":"code","metadata":{"id":"BUTI9EshEfVx"},"source":["checkpoint_path = './models/tmp-checkpoint.h5'\n","checkpointer = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_path,\n","    save_best_only=True,\n","    monitor='loss', \n","    verbose=1, \n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c5zhp3vmZU1l"},"source":["### 문제 14. 모델을 컴파일합니다. 옵티마이저는 adam을 사용해주세요."]},{"cell_type":"code","metadata":{"id":"Ut_xbBU01B1y"},"source":["reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=0.0001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOViuJyyEfVx"},"source":["model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P0VrJdeDZiPn"},"source":["### 문제 15. epoch를 지정합니다."]},{"cell_type":"markdown","metadata":{"id":"gSL8bTGj1B1y"},"source":["**steps_per_epoch에 대하여**\n","\n","\n","1. fit()함수를 취할 때, 버젼별로 `steps_per_epoch`과, `validation_steps`의 값이 지정되어 있지 않으면 학습이 안되는 현상이 있습니다.\n","\n","2. 따라서, 위의 2가지 파라미터에 값을 넣어 주면 정상적으로 학습하기 시작합니다.\n","\n","3. `steps_per_epoch`은 weight를 업데이트 하는 주기 입니다. 보통은 이미지 데이터셋의 총 갯수가 1000개 일 때, `batch_size`가 128개 이면, **이미지갯수**/**배치사이즈**만큼 weight를 업데이트 합니다. 쉽게 말해서, batch가 다 돌때마다 weight를 업데이트 합니다.\n","\n","4. 1000개 이미지 / 128 하면 7.8125 (소수점)이 나오기 때문에 1000 // 128 해주면 해결되는데요. 몫을 구하면 7이나오기 때문에 +1을 해주면 됩니다. 즉, 마지막 batch는 128개보다 적겠네요. 그래도 weight 업데이트 해줘야하니깐 최종 `steps_per_epoch`은 8이 맞습니다.\n","\n","5. validation_steps는 validation_generator의 weight 업데이트 숫자입니다. 4번에서 구한 `steps_per_epoch`과 동일한 원리로 validation dataset을 기준으로 계산하면 됩니다.\n","\n","6. validation data는 500개고, batch_size가 이번에는 32개면 500 // 32 + 1 = 16 이 되겠네요.\n","\n","7. 하지만, 이런 고민 모두다 필요없이 `steps_per_epoch` = len(training_generator)\n","`validation_steps` = len(validation_generator) 로 설정해주면 초 간단합니다^^"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TirOJgNMEfVx","outputId":"3e6a6515-e3ca-403d-b32f-84a7b9cdae1f"},"source":["steps_per_epoch = (len(text_as_int) - window_size) // (batch_size)\n","steps_per_epoch"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1740"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"Cm95kBbfZlf0"},"source":["### 문제 16. 모델을 학습하고 callbacks로 앞에서 만든 체크포인트를 할당해줍니다."]},{"cell_type":"code","metadata":{"id":"UXnkQba11B1z"},"source":["model.load_weights(checkpoint_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"H8nTDb2xEfVy","outputId":"9b7d63ce-20db-4368-c4b1-b9562f3b71ce"},"source":["model.fit(train_data, \n","          epochs=30, \n","          steps_per_epoch=steps_per_epoch,\n","          callbacks=[checkpointer, reduce_lr])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0840 - acc: 0.9789\n","Epoch 00001: loss improved from 0.08437 to 0.08399, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0840 - acc: 0.9789\n","Epoch 2/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0836 - acc: 0.9789\n","Epoch 00002: loss improved from 0.08399 to 0.08357, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 108s 62ms/step - loss: 0.0836 - acc: 0.9789\n","Epoch 3/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0832 - acc: 0.9790\n","Epoch 00003: loss improved from 0.08357 to 0.08323, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0832 - acc: 0.9790\n","Epoch 4/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0829 - acc: 0.9790\n","Epoch 00004: loss improved from 0.08323 to 0.08287, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0829 - acc: 0.9790\n","Epoch 5/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0826 - acc: 0.9791\n","Epoch 00005: loss improved from 0.08287 to 0.08257, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0826 - acc: 0.9791\n","Epoch 6/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0823 - acc: 0.9791\n","Epoch 00006: loss improved from 0.08257 to 0.08229, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 108s 62ms/step - loss: 0.0823 - acc: 0.9791\n","Epoch 7/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0820 - acc: 0.9792\n","Epoch 00007: loss improved from 0.08229 to 0.08197, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0820 - acc: 0.9792\n","Epoch 8/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0817 - acc: 0.9792\n","Epoch 00008: loss improved from 0.08197 to 0.08167, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 108s 62ms/step - loss: 0.0817 - acc: 0.9792\n","Epoch 9/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0814 - acc: 0.9792\n","Epoch 00009: loss improved from 0.08167 to 0.08142, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 108s 62ms/step - loss: 0.0814 - acc: 0.9792\n","Epoch 10/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0811 - acc: 0.9793\n","Epoch 00010: loss improved from 0.08142 to 0.08114, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 108s 62ms/step - loss: 0.0811 - acc: 0.9793\n","Epoch 11/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0809 - acc: 0.9793\n","Epoch 00011: loss improved from 0.08114 to 0.08091, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0809 - acc: 0.9793\n","Epoch 12/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0806 - acc: 0.9793\n","Epoch 00012: loss improved from 0.08091 to 0.08065, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 108s 62ms/step - loss: 0.0806 - acc: 0.9793\n","Epoch 13/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0804 - acc: 0.9794\n","Epoch 00013: loss improved from 0.08065 to 0.08044, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 108s 62ms/step - loss: 0.0804 - acc: 0.9794\n","Epoch 14/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0802 - acc: 0.9794\n","Epoch 00014: loss improved from 0.08044 to 0.08021, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0802 - acc: 0.9794\n","Epoch 15/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0801 - acc: 0.9794\n","Epoch 00015: loss improved from 0.08021 to 0.08005, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0801 - acc: 0.9794\n","Epoch 16/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0798 - acc: 0.9795\n","Epoch 00016: loss improved from 0.08005 to 0.07982, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0798 - acc: 0.9795\n","Epoch 17/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0796 - acc: 0.9795\n","Epoch 00017: loss improved from 0.07982 to 0.07961, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0796 - acc: 0.9795\n","Epoch 18/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0794 - acc: 0.9795\n","Epoch 00018: loss improved from 0.07961 to 0.07942, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0794 - acc: 0.9795\n","Epoch 19/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0793 - acc: 0.9795\n","Epoch 00019: loss improved from 0.07942 to 0.07927, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0793 - acc: 0.9795\n","Epoch 20/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0791 - acc: 0.9796\n","Epoch 00020: loss improved from 0.07927 to 0.07908, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 108s 62ms/step - loss: 0.0791 - acc: 0.9796\n","Epoch 21/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0789 - acc: 0.9796\n","Epoch 00021: loss improved from 0.07908 to 0.07893, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0789 - acc: 0.9796\n","Epoch 22/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0788 - acc: 0.9796\n","Epoch 00022: loss improved from 0.07893 to 0.07880, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0788 - acc: 0.9796\n","Epoch 23/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0786 - acc: 0.9797\n","Epoch 00023: loss improved from 0.07880 to 0.07862, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0786 - acc: 0.9797\n","Epoch 24/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0785 - acc: 0.9797\n","Epoch 00024: loss improved from 0.07862 to 0.07852, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0785 - acc: 0.9797\n","Epoch 25/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0783 - acc: 0.9797\n","Epoch 00025: loss improved from 0.07852 to 0.07834, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0783 - acc: 0.9797\n","Epoch 26/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0782 - acc: 0.9797\n","Epoch 00026: loss improved from 0.07834 to 0.07819, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0782 - acc: 0.9797\n","Epoch 27/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0781 - acc: 0.9798\n","Epoch 00027: loss improved from 0.07819 to 0.07810, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 108s 62ms/step - loss: 0.0781 - acc: 0.9798\n","Epoch 28/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0780 - acc: 0.9798\n","Epoch 00028: loss improved from 0.07810 to 0.07796, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 108s 62ms/step - loss: 0.0780 - acc: 0.9798\n","Epoch 29/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0778 - acc: 0.9798\n","Epoch 00029: loss improved from 0.07796 to 0.07779, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0778 - acc: 0.9798\n","Epoch 30/30\n","1740/1740 [==============================] - ETA: 0s - loss: 0.0777 - acc: 0.9798\n","Epoch 00030: loss improved from 0.07779 to 0.07765, saving model to ./models/sample-model.h5\n","1740/1740 [==============================] - 107s 62ms/step - loss: 0.0777 - acc: 0.9798\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f9de443c160>"]},"metadata":{"tags":[]},"execution_count":124}]},{"cell_type":"markdown","metadata":{"id":"7P__1XcrbXTY"},"source":["## Step 5. 모델을 활용한 뉴스기사 생성"]},{"cell_type":"code","metadata":{"id":"q9B_hZHcEfVy"},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                              batch_input_shape=[1, None]),\n","    tf.keras.layers.LSTM(rnn_units,\n","                         return_sequences=True,\n","                         stateful=True,\n","                         recurrent_initializer='glorot_uniform'),\n","    tf.keras.layers.Dense(vocab_size)\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kYMYNo_naEfY"},"source":["### 문제 17. 저장한 Model Checkpoint를 불러옵니다."]},{"cell_type":"code","metadata":{"id":"VzusyCTQEfVy"},"source":["model.load_weights(checkpoint_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wqvfa3m8bF15"},"source":["### 문제 18. 모델을 build하고 요약 내용을 출력해봅니다."]},{"cell_type":"code","metadata":{"id":"6Ynbz1aZEfVy"},"source":["model.build(tf.TensorShape([1, None]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Q45AWkOEfVz","outputId":"7ebb554d-4edd-4520-9cc9-27fc83f137d4"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_6 (Embedding)      (1, None, 256)            300288    \n","_________________________________________________________________\n","lstm_6 (LSTM)                (1, None, 1024)           5246976   \n","_________________________________________________________________\n","dense_6 (Dense)              (1, None, 1173)           1202325   \n","=================================================================\n","Total params: 6,749,589\n","Trainable params: 6,749,589\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TQUD6gPKbPBT"},"source":["### 문제 19. 불러온 모델을 활용해 뉴스기사를 생성해봅니다."]},{"cell_type":"code","metadata":{"id":"Bl6eJ0-IEfVz"},"source":["def generate_text(model, start_string):\n","    # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)\n","\n","    # 생성할 문자의 수\n","    num_generate = 1000\n","\n","    # 시작 문자열을 숫자로 변환(벡터화)\n","    input_eval = [char2idx[s] for s in start_string]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    # 여기에서 배치 크기 == 1\n","    model.reset_states()\n","    for i in range(num_generate):\n","        predictions = model(input_eval)\n","        # 배치 차원 제거\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        # 예측된 단어를 다음 입력으로 모델에 전달\n","        # 이전 은닉 상태와 함께\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","        result_char = idx2char[predicted_id]\n","        \n","        # '#' 문자열을 만나면 종료합니다.\n","        if result_char == '#':\n","            break\n","        \n","        text_generated.append(result_char)\n","\n","    return (start_string + ''.join(text_generated))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3IcCpGJEfVz","outputId":"fe93cea8-0f8d-4f91-989d-a6b1c50e4f87"},"source":["print(generate_text(model, start_string=u\"스마트폰 \"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["스마트폰 관계자는 불법 사이트가 마치 정식 웹툰 플랫폼처럼 모든 웹툰을 긁어다가 공개하고 유료로 제공하는 웹툰도 공짜로 풀어서 작품 가치를 헐값으로 만들고 있다면서 지난해 밤토끼와 같은 불법 사이트로 인해 매출이 줄었다 작가들이 공들여 만든 작품이 성인광고로 도배된 사이트에서 미끼상품으로 공개되는 것에 큰 수모를 느끼고 있다고 말했다6일 웹툰 업계에 따르면 현재 한국 웹툰을 불법으로 연재하는 불법 복제 사이트 200여 곳이 운영되고 있다 이 중 가장 큰 불법 사이트는 밤토끼다 지난해 1월부터 본격 운영된 이곳은 네이버카카오레진코믹스짬툰 등 주요 웹툰 1500편을 무단으로 복사해 제공한다 정식 웹툰 플랫폼에 공개된 신작이 두 시간도 안돼 이곳에 올라오는 식이다 밤토끼는 1년 만에 국내 최대 웹툰 플랫폼 네이버를 앞지르기 시작했다 닐슨코리안클릭에 따르면 밤토끼 월 페이지뷰는 1억3709만건지난해 12월 기준으로 네이버1억2081만건를 제쳤다 밤토끼는 국내 경찰의 수사망이 미치지 못하는 해외에 서버를 두고 성매매 성인용품 도박 등 불법 사이트 광고주로부터 수익을 챙기고 있다레진코믹스에 따르면 밤토끼 사이트 운영사는 현재 중앙아메리카 소국 벨리즈로 돼 있다 그러나 이 회사는 우편 사서함 주소만 있는 유령회사이고 인터넷 접속을 제공하는ISP업체는 불가리아에 있는 회사를 사용하며 자료를 저장하는 데이터센터는 우크라이나 업체를 사용한다 레진엔터테인먼트 관계자는 벨리즈 불가리아 업체와 우크라이나 소재 데이터센터에 서비스 차단을 요청했지만 1년 넘게 어떠한 답변도 받지 못했다고 말했다정부도 밤토끼 차단을 시도했지만 번번이 실패했다저작권 권리자가 권리 침해를 신고하면 한국저작권보호원의 심사를 거쳐 방송통신심의원회가ISP업체에 불법 사이트 차단을 요청한다 사이트를 폐쇄할 수는 없지만 국내 이용자 접속을 차단함으로써 직접 피해를 줄이는 방식이다 이때ISP업체가 사용하는 차단 기술은 데이터 신호를 암호화하지 않은 통신 규약 해도적인 재조를 유발하는 실리콘 웨이퍼 시장서를 불법 사용한 것으로 \n"],"name":"stdout"}]}]}