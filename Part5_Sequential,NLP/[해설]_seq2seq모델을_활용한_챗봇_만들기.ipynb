{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "[해설] seq2seq모델을 활용한 챗봇 만들기",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ggNKq4cdxDr"
      },
      "source": [
        "# Seq2Seq 모델을 활용한 챗봇 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iay72EBedxDs"
      },
      "source": [
        "## STEP 1. Seq2Seq 모델의 개요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5OoHUgCgw2F"
      },
      "source": [
        "### 문제 01. seq2seq 모델의 구조에 대한 이해\r\n",
        "\r\n",
        "- encoder & decoder 구조\r\n",
        "- 데이터 셋의 구성\r\n",
        "- context vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTMGVydFdxDs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "82094275-c525-47e5-b1a2-e68b1abf8e12"
      },
      "source": [
        "from IPython.display import Image\r\n",
        "\r\n",
        "Image('https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBwHP5uOdxDs"
      },
      "source": [
        "### 문제 02. 데이터셋에 필요한 라이브러리를 다운로드 받습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft3-FND8dxDt"
      },
      "source": [
        "`Korpora`는 한글 자연어처리 데이터)셋입니다.\n",
        "\n",
        "- [깃헙 주소 링크](https://github.com/ko-nlp/Korpora)\n",
        "- [공식 도큐먼트](https://pypi.org/project/Korpora/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4abEgfIdxDt"
      },
      "source": [
        "설치 명령어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2riYkfBdxDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5ebfe5-7293-451a-daa0-21d5788098f7"
      },
      "source": [
        "!pip install Korpora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Korpora\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b1/5e563e23f1f705574bbeb55555e0cb95c9813e9396d654cd42709418ab66/Korpora-0.2.0-py3-none-any.whl (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 21.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.5MB/s \n",
            "\u001b[?25hCollecting xlrd>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/0c/c2a72d51fe56e08a08acc85d13013558a2d793028ae7385448a6ccdfae64/xlrd-2.0.1-py2.py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.7MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.46.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/02/8f8880a4fd6625461833abcf679d4c12a44c76f9925f92bf212bb6cefaad/tqdm-4.56.0-py2.py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from Korpora) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from Korpora) (1.19.5)\n",
            "Requirement already satisfied: dataclasses>=0.6 in /usr/local/lib/python3.6/dist-packages (from Korpora) (0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->Korpora) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->Korpora) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->Korpora) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20.0->Korpora) (2.10)\n",
            "Installing collected packages: xlrd, tqdm, Korpora\n",
            "  Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed Korpora-0.2.0 tqdm-4.56.0 xlrd-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMa5kXSrdxDt"
      },
      "source": [
        "- 이 중 챗봇용 데이터셋인 `KoreanChatbotKorpus`를 다운로드 받습니다.\n",
        "- `KoreanChatbotKorpus` 데이터셋을 활용하여 챗봇 모델을 학습합니다.\n",
        "- text, pair로 구성되어 있습니다.\n",
        "- 질의는 **text**, 답변은 **pair**입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPdBMDJlwHOS"
      },
      "source": [
        "### 문제 03. Korpora의 챗봇 데이터를 import 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ1yROOodxDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ae85ee5-de54-4821-9efa-65dea92890af"
      },
      "source": [
        "from Korpora import KoreanChatbotKorpus\n",
        "\n",
        "# 코드를 입력하세요\n",
        "corpus = KoreanChatbotKorpus()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[korean_chatbot_data] download ChatbotData.csv: 893kB [00:00, 14.3MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : songys@github\n",
            "    Repository : https://github.com/songys/Chatbot_data\n",
            "    References :\n",
            "\n",
            "    Chatbot_data_for_Korean v1.0\n",
            "      1. 챗봇 트레이닝용 문답 페어 11,876개\n",
            "      2. 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5opBY8yrdxDu"
      },
      "source": [
        "예시 텍스트를 보면 구어체로 구성되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPy_FY_bdxDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b20f31e-d528-41f9-8daf-45dc837f4b1f"
      },
      "source": [
        "corpus.get_all_texts()[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['12시 땡!',\n",
              " '1지망 학교 떨어졌어',\n",
              " '3박4일 놀러가고 싶다',\n",
              " '3박4일 정도 놀러가고 싶다',\n",
              " 'PPL 심하네',\n",
              " 'SD카드 망가졌어',\n",
              " 'SD카드 안돼',\n",
              " 'SNS 맞팔 왜 안하지ㅠㅠ',\n",
              " 'SNS 시간낭비인 거 아는데 매일 하는 중',\n",
              " 'SNS 시간낭비인데 자꾸 보게됨']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trUKeRRrdxDu"
      },
      "source": [
        "데이터셋 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvPI1QwcNtGf",
        "outputId": "a76a2e9a-e475-4933-c37d-2c29b1eb3d14"
      },
      "source": [
        "corpus.get_all_pairs()[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', '여행은 언제나 좋죠.', '눈살이 찌푸려지죠.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhRYxdShNnjC",
        "outputId": "69f12711-effd-49d2-fada-37a99cdffc08"
      },
      "source": [
        "corpus.get_all_texts()[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qkxF9uEdxDv"
      },
      "source": [
        "## STEP 2. 데이터 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8ri7fDUwO8c"
      },
      "source": [
        "### 문제 04. quenstion 데이터셋과 answer 데이터셋 분리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKWVS48VdxDv"
      },
      "source": [
        "**question**과 **answer**를 분리합니다.\n",
        "\n",
        "**question**은 질의로 활용될 데이터셋, **answer**는 답변으로 활용될 데이터 셋입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yne75jOgdxDv"
      },
      "source": [
        "texts = []\n",
        "pairs = []\n",
        "\n",
        "# 코드를 입력하세요\n",
        "for text, pair in zip(corpus.get_all_texts(), corpus.get_all_pairs()):\n",
        "    texts.append(text)\n",
        "    pairs.append(pair)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAKrBxD1dxDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859faa5e-426a-4fc2-8246-9f434bfc8299"
      },
      "source": [
        "list(zip(texts, pairs))[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('12시 땡!', '하루가 또 가네요.'),\n",
              " ('1지망 학교 떨어졌어', '위로해 드립니다.'),\n",
              " ('3박4일 놀러가고 싶다', '여행은 언제나 좋죠.'),\n",
              " ('3박4일 정도 놀러가고 싶다', '여행은 언제나 좋죠.'),\n",
              " ('PPL 심하네', '눈살이 찌푸려지죠.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMqdaHg4dxDv"
      },
      "source": [
        "### 문제 05. 특수문자는 제거합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXxfmSf_dxDw"
      },
      "source": [
        "**한글과 숫자를 제외한 특수문자를 제거**하도록 합니다.\n",
        "\n",
        "\n",
        "[참고] 튜토리얼에서는 특수문자와 영문자를 제거하나, 실제 프로젝트에 적용해보기 위해서는 신중히 결정해야합니다\n",
        "\n",
        "*챗봇 대화에서 영어도 많이 사용되고, 특수문자도 굉장히 많이 사용됩니다. 따라서, 선택적으로 제거할 특수기호나 영문자를 정의한 후에 전처리를 진행하야합니다.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQGtKQL3dxDw"
      },
      "source": [
        "# re 모듈은 regex expression을 적용하기 위하여 활용합니다.\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ociO_uPCdxDw"
      },
      "source": [
        "def clean_sentence(sentence):\n",
        "    # 한글, 숫자를 제외한 모든 문자는 제거합니다.\n",
        "    # 코드를 입력하세요\n",
        "    sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]',r'', sentence)\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viW_yTWYdxDw"
      },
      "source": [
        "**적용한 예시**\n",
        "\n",
        "한글, 숫자 이외의 모든 문자를 전부 제거됨을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8addNjEkdxDw"
      },
      "source": [
        "clean_sentence('12시 땡^^!??')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luGFOsVMdxDw"
      },
      "source": [
        "clean_sentence('abcef가나다^^$%@12시 땡^^!??')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TYCMUC5dxDx"
      },
      "source": [
        "### 문제 06. 한글 형태소 분석기 (Konlpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5jA0mZEdxDx"
      },
      "source": [
        "형태소 분석기를 활용하여 문장을 분리합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owSMeXN3dxDx"
      },
      "source": [
        "```가방에 들어가신다 -> 가방/NNG + 에/JKM + 들어가/VV + 시/EPH + ㄴ다/EFN```\n",
        "\n",
        "- **형태소 분석** 이란 형태소를 비롯하여, 어근, 접두사/접미사, 품사(POS, part-of-speech) 등 다양한 언어적 속성의 구조를 파악하는 것입니다.\n",
        "- **konlpy 형태소 분석기를 활용**하여 한글 문장에 대한 토큰화처리를 보다 효율적으로 처리합니다.\n",
        "\n",
        "\n",
        "\n",
        "[공식 도큐먼트](https://konlpy-ko.readthedocs.io/ko/v0.4.3/morph/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az7bZjTmdxDx"
      },
      "source": [
        "**설치**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5TvyZSXdxDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1509c57a-cafc-4726-ac74-19724ca4c2eb"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 19.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.0MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 62.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: colorama, beautifulsoup4, JPype1, tweepy, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64h11Yq2dxDx"
      },
      "source": [
        "konlpy 내부에는 Kkma, Okt, Twitter 등등의 형태소 분석기가 존재하지만, 이번 튜토리얼에서는 Okt를 활용하도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otm0rw1HdxDy"
      },
      "source": [
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YQyXtGNdxDy"
      },
      "source": [
        "# Okt 토크나이저 객체를 생성합니다.\r\n",
        "# 코드를 입력하세요\r\n",
        "okt = Okt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iab8xa-PdxDy"
      },
      "source": [
        "# 형태소 변환에 활용하는 함수\n",
        "# morphs 함수 안에 변환한 한글 문장을 입력 합니다.\n",
        "def process_morph(sentence):\n",
        "    # 코드를 입력 하세요\n",
        "    return ' '.join(okt.morphs(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "ykaC0FJ5Rl4_",
        "outputId": "a6398329-3239-4160-bd18-275e9dffc6d8"
      },
      "source": [
        "process_morph('한글은 홀소리와 닿소리 모두 소리틀을 본떠 만든 음소문자[1]로 한글 맞춤법에서는 닿소리 14개와 홀소리 10개, 모두 24개를 표준으로 삼는다. \"나랏말이 중국과 달라\" 문제를 느낀 조선의 세종대왕이 한국어는 물론 이웃나라 말까지 나타내도록 1443년 창제하여 1446년 반포하였다. ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'한글 은 홀소리 와 닿소리 모두 소 리틀 을 본떠 만든 음소문자 [ 1 ] 로 한글 맞춤법 에서는 닿소리 14 개 와 홀소리 10 개 , 모두 24 개 를 표준 으로 삼는다 . \" 나랏말 이 중국 과 달라 \" 문제 를 느낀 조선 의 세종대왕 이 한국어 는 물론 이웃 나라 말 까지 나타내도록 1443년 창제 하여 1446년 반포 하였다 .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJiMmY55w7VK"
      },
      "source": [
        "## STEP 3. 데이터셋 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TSofap7xBuz"
      },
      "source": [
        "### 문제 07. Seq2Seq 학습을 위한 데이터셋 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfJ9TiNldxDy"
      },
      "source": [
        "**Seq2Seq** 모델이 학습하기 위한 데이터셋을 구성할 때, 다음과 같이 **3가지 데이터셋**을 구성합니다.\n",
        "\n",
        "- `question`: encoder input 데이터셋 (질의 전체)\n",
        "- `answer_input`: decoder input 데이터셋 (답변의 시작). START 토큰을 문장 처음에 추가 합니다.\n",
        "- `answer_output`: decoder output 데이터셋 (답변의 끝). END 토큰을 문장 마지막에 추가 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFV7yQggdxDy"
      },
      "source": [
        "def clean_and_morph(sentence, is_question=True):\n",
        "    # 한글 문장 전처리\n",
        "    # 코드를 입력하세요\n",
        "    sentence = clean_sentence(sentence)\n",
        "    # 형태소 변환\n",
        "    # 코드를 입력하세요\n",
        "    sentence = process_morph(sentence)\n",
        "    # Question 인 경우, Answer인 경우를 분기하여 처리합니다.\n",
        "    # 코드를 입력하세요\n",
        "    if is_question:\n",
        "        return sentence\n",
        "    else:\n",
        "        # START 토큰은 decoder input에 END 토큰은 decoder output에 추가합니다.\n",
        "        # 코드를 입력하세요\n",
        "        return ('<START> ' + sentence, sentence + ' <END>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S_SjPQ5xOy6"
      },
      "source": [
        "### 문제 08. preprocess 함수에서는 `text`와 `pair`에 대한 데이터 셋 구성을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgnFV1AdxDz"
      },
      "source": [
        "def preprocess(texts, pairs):\n",
        "    questions = []\n",
        "    answer_in = []\n",
        "    answer_out = []\n",
        "\n",
        "    # 질의에 대한 전처리\n",
        "    for text in texts:\n",
        "        # 전처리와 morph 수행\n",
        "        # 코드를 입력하세요\n",
        "        question = clean_and_morph(text, is_question=True)\n",
        "        questions.append(question)\n",
        "\n",
        "    # 답변에 대한 전처리\n",
        "    for pair in pairs:\n",
        "        # 전처리와 morph 수행\n",
        "        # 코드를 입력하세요\n",
        "        in_, out_ = clean_and_morph(pair, is_question=False)\n",
        "        answer_in.append(in_)\n",
        "        answer_out.append(out_)\n",
        "    \n",
        "    return questions, answer_in, answer_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPQzKJnqdxDz"
      },
      "source": [
        "questions, answer_in, answer_out = preprocess(texts, pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R1v4qThdxDz"
      },
      "source": [
        "questions[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P-wlhGPdxDz"
      },
      "source": [
        "answer_in[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAiUkA9OdxDz"
      },
      "source": [
        "answer_out[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx6G3-32xc6w"
      },
      "source": [
        "### 문제 09. `all_sentences` 변수에 모든 데이터셋 문장을 합칩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc3aTpP_xja-"
      },
      "source": [
        "합치는 이유는 모든 문장을 합쳐서 토큰화를 진행하기 위함입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM0Z-lEAdxDz"
      },
      "source": [
        "# 코드를 입력하세요\r\n",
        "all_sentences = questions + answer_in + answer_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6RirVQPdxDz"
      },
      "source": [
        "# 코드를 입력하세요\n",
        "a = (' '.join(questions) + ' '.join(answer_in) + ' '.join(answer_out)).split()\n",
        "len(set(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9VrosgvdxD0"
      },
      "source": [
        "## STEP 4. 토큰화 (Tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxgX7BzSdxD0"
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# WARNING 무시\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imZynPXTx7KX"
      },
      "source": [
        "### 문제 10. 토근의 option을 정의 합니다.\r\n",
        "\r\n",
        "- filter는 ''로 지정합니다. \r\n",
        "- lower는 False로 지정합니다.\r\n",
        "- oov_token은 '<OOV>'로 지정합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpHQZgXNdxD0"
      },
      "source": [
        "**토큰의 정의**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T64bShSydxD0"
      },
      "source": [
        "# 코드를 입력하세요\r\n",
        "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXX-jHRdyOP5"
      },
      "source": [
        "### 문제 11. 단어 사전을 만듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SglFhzjdxD0"
      },
      "source": [
        "**Tokenizer**로 문장에 대한 Word-Index Vocabulary(단어 사전)을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Jm3j8YadxD0"
      },
      "source": [
        "# 코드를 입력하세요\r\n",
        "tokenizer.fit_on_texts(all_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ5t4QKydxD0"
      },
      "source": [
        "**단어 사전 10개 출력**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9e3ZjBudxD1"
      },
      "source": [
        "for word, idx in tokenizer.word_index.items():\n",
        "    print(f'{word}\\t\\t => \\t{idx}')\n",
        "    if idx > 10:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKRMzy-y0GA8"
      },
      "source": [
        "### 문제 12. 토큰의 갯수 지정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auMa_FfzdxD1"
      },
      "source": [
        "**토큰의 갯수 확인**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk7Rfl3AdxD1"
      },
      "source": [
        "len(tokenizer.word_index)\r\n",
        "# VOCAB_SIZE 변수에 토큰의 단어 사전의 갯수를 입력합니다.\r\n",
        "# 코드를 입력하세요\r\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybduw-HedxD1"
      },
      "source": [
        "### 문제 13. 치환: 텍스트를 시퀀스로 인코딩 (`texts_to_sequences`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV0p7p7VdxD1"
      },
      "source": [
        "|# 코드를 입력하세요\n",
        "question_sequence = tokenizer.texts_to_sequences(questions)\n",
        "answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n",
        "answer_out_sequence = tokenizer.texts_to_sequences(answer_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y1HNPGndxD1"
      },
      "source": [
        "### 문제 14. 문장의 길이 맞추기 (`pad_sequences`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp3eupZgyliG"
      },
      "source": [
        "`pad_sequences`를 활용하여 문장의 길이를 맞춰 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdhUgUc_dxD1"
      },
      "source": [
        "# 최대 문자의 토큰 갯수는 30개로 지정합니다.\r\n",
        "MAX_LENGTH = 30\r\n",
        "TRUNCATING = 'post'\r\n",
        "PADDING = 'post'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1VwlFM3dxD2"
      },
      "source": [
        "# 코드를 입력 하세요\n",
        "question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "answer_in_padded = pad_sequences(answer_in_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)\n",
        "answer_out_padded = pad_sequences(answer_out_sequence, maxlen=MAX_LENGTH, truncating=TRUNCATING, padding=PADDING)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL4SGOghy3gD"
      },
      "source": [
        "변환된 데이터 셋의 shape를 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F01y9olHdxD2"
      },
      "source": [
        "question_padded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0siuIXzdxD2"
      },
      "source": [
        "answer_in_padded.shape, answer_out_padded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMY1xxFa0hUg"
      },
      "source": [
        "## STEP 5. 데이터셋 변환 함수 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv2MLohkdxD4"
      },
      "source": [
        "### 문제 15. 단어별 원핫인코딩 적용\n",
        "\n",
        "단어별 원핫인코딩을 적용하는 이유는 decoder의 output(출력)을 원핫인코딩 vector로 변환하기 위함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pe6My-e6dxD4"
      },
      "source": [
        "def convert_to_one_hot(padded):\n",
        "    # 원핫인코딩 초기화\n",
        "    one_hot_vector = np.zeros((len(answer_out_padded), MAX_LENGTH, VOCAB_SIZE))\n",
        "\n",
        "    # 디코더 목표를 원핫인코딩으로 변환\n",
        "    # 학습시 입력은 인덱스이지만, 출력은 원핫인코딩 형식임\n",
        "    for i, sequence in enumerate(answer_out_padded):\n",
        "        for j, index in enumerate(sequence):\n",
        "            one_hot_vector[i, j, index] = 1\n",
        "\n",
        "    return one_hot_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWU_9by8dxD4"
      },
      "source": [
        "# RAM 용량이 초과하는 경우 세션이 다운될 수 있습니다.\n",
        "# 그럴때는 학습 세트의 전체 사이즈를 줄여주세요.\n",
        "answer_in_one_hot = convert_to_one_hot(answer_in_padded)\n",
        "answer_out_one_hot = convert_to_one_hot(answer_out_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FozfIPBTdxD4",
        "outputId": "a42bf9f2-d106-4e76-8f66-f0b0fec2dfbb"
      },
      "source": [
        "answer_in_one_hot[0].shape, answer_in_one_hot[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((30, 12638), (30, 12638))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FpX53_MdxD5"
      },
      "source": [
        "### 문제 16. 변환된 index를 다시 단어로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Djs6RsadxD5"
      },
      "source": [
        "def convert_index_to_text(indexs, end_token): \n",
        "    \n",
        "    sentence = ''\n",
        "    \n",
        "    # 모든 문장에 대해서 반복\n",
        "    for index in indexs:\n",
        "        if index == end_token:\n",
        "            # 끝 단어이므로 예측 중비\n",
        "            break;\n",
        "        # 사전에 존재하는 단어의 경우 단어 추가\n",
        "        if index > 0 and tokenizer.index_word[index] is not None:\n",
        "            sentence += tokenizer.index_word[index]\n",
        "        else:\n",
        "        # 사전에 없는 인덱스면 빈 문자열 추가\n",
        "            sentence += ''\n",
        "            \n",
        "        # 빈칸 추가\n",
        "        sentence += ' '\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf_g3T1RdxD2"
      },
      "source": [
        "## STEP 6. 모델 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kY98vN5zCkl"
      },
      "source": [
        "모델 생성을 위한 필요한 모듈을 import 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A7JPC6OdxD2"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3XQHILadxD2"
      },
      "source": [
        "### 문제 17. 학습용 인코더 (Encoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_UTWGaodxD2"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Encoder, self).__init__()\n",
        "        # 코드를 입력하세요\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.lstm = LSTM(units, return_state=True)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        # 코드를 입력하세요\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x)\n",
        "        return [hidden_state, cell_state]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4hFzCh-dxD3"
      },
      "source": [
        "### 문제 18. 학습용 디코더 (Decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WW_fADJdxD3"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Decoder, self).__init__()\n",
        "        # 코드를 입력하세요\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim, input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.lstm = LSTM(units, \n",
        "                         return_state=True, \n",
        "                         return_sequences=True, \n",
        "                        )\n",
        "        self.dense = Dense(vocab_size, activation='softmax')\n",
        "    \n",
        "    def call(self, inputs, initial_state):\n",
        "        # 코드를 입력하세요\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)        \n",
        "        x = self.dense(x)\n",
        "        return x, hidden_state, cell_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmXc7Jw8dxD3"
      },
      "source": [
        "### 문제 19. Seq2Seq 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcGmmcY_zUx3"
      },
      "source": [
        "Seq2Seq 모델은 사전 단계에서 만들어놓은 encoder와 decoder를 활용하여 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUx38fcsdxD3"
      },
      "source": [
        "class Seq2Seq(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        # 코드를 입력하세요\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.time_steps = time_steps\n",
        "        \n",
        "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        \n",
        "    def call(self, inputs, training=True):\n",
        "        if training:\n",
        "            # 코드를 입력하세요\n",
        "            encoder_inputs, decoder_inputs = inputs\n",
        "            context_vector = self.encoder(encoder_inputs)\n",
        "            decoder_outputs, _, _ = self.decoder(inputs=decoder_inputs, initial_state=context_vector)\n",
        "            return decoder_outputs\n",
        "        else:\n",
        "            # 코드를 입력하세요\n",
        "            context_vector = self.encoder(inputs)\n",
        "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
        "            results = tf.TensorArray(tf.int32, self.time_steps)\n",
        "            \n",
        "            for i in tf.range(self.time_steps):\n",
        "                decoder_output, decoder_hidden, decoder_cell = self.decoder(target_seq, initial_state=context_vector)\n",
        "                decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32)\n",
        "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "                results = results.write(i, decoder_output)\n",
        "                \n",
        "                if decoder_output == self.end_token:\n",
        "                    break\n",
        "                    \n",
        "                target_seq = decoder_output\n",
        "                context_vector = [decoder_hidden, decoder_cell]\n",
        "                \n",
        "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFULTBxcdxD5"
      },
      "source": [
        "## STEP 7. 학습 (Training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTFXrO2T06f4"
      },
      "source": [
        "### 문제 20. 학습을 위한 parameter를 정의하고 체크포인트를 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_FzbsGvdxD6"
      },
      "source": [
        "**하이퍼 파라미터 정의**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noiy8uJ-dxD6"
      },
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 16\n",
        "EMBEDDING_DIM = 100\n",
        "TIME_STEPS = MAX_LENGTH\n",
        "\n",
        "# 코드를 입력하세요\n",
        "START_TOKEN = tokenizer.word_index['<START>']\n",
        "END_TOKEN = tokenizer.word_index['<END>']\n",
        "\n",
        "UNITS = 128\n",
        "\n",
        "# 코드를 입력하세요\n",
        "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
        "DATA_LENGTH = len(questions)\n",
        "SAMPLE_SIZE = 3\n",
        "NUM_EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6jJEq-mdxD6"
      },
      "source": [
        "**체크포인트 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUMtSpCldxD6"
      },
      "source": [
        "checkpoint_path = 'sample-checkpoint.h5'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                             save_best_only=True, \n",
        "                             monitor='loss', \n",
        "                             verbose=1\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQRNjsiCdxD6"
      },
      "source": [
        "### 문제 21. 모델 생성 & compile을 진행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccl3jF-w1eQe"
      },
      "source": [
        "optimizer는 `adam`, loss는 `categorical_crossentropy`로 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DSNFivkdxD7",
        "outputId": "af0e86cf-0402-4d2a-d9d0-e3c102eeca84"
      },
      "source": [
        "# 코드를 입력하세요\n",
        "seq2seq = Seq2Seq(UNITS, VOCAB_SIZE, EMBEDDING_DIM, TIME_STEPS, START_TOKEN, END_TOKEN)\n",
        "seq2seq.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "분산환경 사용 >> GPU: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KepR0JJOdxD7"
      },
      "source": [
        "# (필요시) 연속하여 학습시 체크포인트를 로드하여 이어서 학습합니다.\n",
        "# seq2seq.load_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GpyZ2SH1q_Q"
      },
      "source": [
        "### 문제 22. make_prediction 함수를 정의 합니다. 예측된 결과를 역 출력 해주기 위한 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YuyKYV3dxD7"
      },
      "source": [
        "def make_prediction(model, question_inputs):\n",
        "    results = model(inputs=question_inputs, training=False)\n",
        "    # 변환된 인덱스를 문장으로 변환\n",
        "    # 코드를 입력하세요\n",
        "    results = np.asarray(results).reshape(-1)\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE6aDbXMdxD7",
        "outputId": "4819c738-4e18-4996-d69e-969694ac8852"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
        "    # model.fit(): 학습\n",
        "    # 코드를 입력하세요\n",
        "    seq2seq.fit([question_padded, answer_in_padded],\n",
        "                answer_out_one_hot,\n",
        "                epochs=10,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=[checkpoint]\n",
        "               )\n",
        "    # 랜덤한 샘플 번호 추출\n",
        "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
        "\n",
        "    # 예측 성능 테스트\n",
        "    for idx in samples:\n",
        "        question_inputs = question_padded[idx]\n",
        "        # 문장 예측\n",
        "        results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
        "        \n",
        "        # 변환된 인덱스를 문장으로 변환\n",
        "        results = convert_index_to_text(results, END_TOKEN)\n",
        "        \n",
        "        print(f'Q: {questions[idx]}')\n",
        "        print(f'A: {results}\\n')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing epoch: 1...\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
            "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
            "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1\n",
            "WARNING:tensorflow:Efficient allreduce is not supported for 2 IndexedSlices\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "370/370 [==============================] - ETA: 0s - loss: 1.9503 - acc: 0.8001\n",
            "Epoch 00001: loss improved from inf to 1.95035, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 1.9503 - acc: 0.8001\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 2/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 1.2335 - acc: 0.8242\n",
            "Epoch 00002: loss improved from 1.95035 to 1.23354, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 1.2335 - acc: 0.8242\n",
            "Epoch 3/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 1.1606 - acc: 0.8299\n",
            "Epoch 00003: loss improved from 1.23354 to 1.15987, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 1.1599 - acc: 0.8300\n",
            "Epoch 4/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 1.1127 - acc: 0.8345\n",
            "Epoch 00004: loss improved from 1.15987 to 1.11258, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 1.1126 - acc: 0.8346\n",
            "Epoch 5/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 1.0734 - acc: 0.8382\n",
            "Epoch 00005: loss improved from 1.11258 to 1.07332, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 1.0733 - acc: 0.8382\n",
            "Epoch 6/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 1.0335 - acc: 0.8430\n",
            "Epoch 00006: loss improved from 1.07332 to 1.03355, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 1.0335 - acc: 0.8430\n",
            "Epoch 7/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.9934 - acc: 0.8472\n",
            "Epoch 00007: loss improved from 1.03355 to 0.99330, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.9933 - acc: 0.8472\n",
            "Epoch 8/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.9560 - acc: 0.8505\n",
            "Epoch 00008: loss improved from 0.99330 to 0.95586, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.9559 - acc: 0.8506\n",
            "Epoch 9/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.9218 - acc: 0.8538\n",
            "Epoch 00009: loss improved from 0.95586 to 0.92182, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.9218 - acc: 0.8538\n",
            "Epoch 10/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.8916 - acc: 0.8565\n",
            "Epoch 00010: loss improved from 0.92182 to 0.89157, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.8916 - acc: 0.8565\n",
            "Q: 긴 시간 이 흐른 후 면 괜찮아지겠지\n",
            "A: 잘 않아요 \n",
            "\n",
            "\n",
            "Q: 정말 다시 돌아 온다면\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 더 알 고 싶어 지는 사람 이야\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 11...\n",
            "Epoch 1/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.8634 - acc: 0.8593\n",
            "Epoch 00001: loss improved from 0.89157 to 0.86313, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.8631 - acc: 0.8594\n",
            "Epoch 2/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.8371 - acc: 0.8617\n",
            "Epoch 00002: loss improved from 0.86313 to 0.83711, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.8371 - acc: 0.8617\n",
            "Epoch 3/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.8123 - acc: 0.8640\n",
            "Epoch 00003: loss improved from 0.83711 to 0.81240, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.8124 - acc: 0.8640\n",
            "Epoch 4/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.7898 - acc: 0.8659\n",
            "Epoch 00004: loss improved from 0.81240 to 0.78967, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.7897 - acc: 0.8659\n",
            "Epoch 5/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.7681 - acc: 0.8680\n",
            "Epoch 00005: loss improved from 0.78967 to 0.76797, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.7680 - acc: 0.8680\n",
            "Epoch 6/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.7477 - acc: 0.8699\n",
            "Epoch 00006: loss improved from 0.76797 to 0.74748, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.7475 - acc: 0.8699\n",
            "Epoch 7/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.7272 - acc: 0.8723\n",
            "Epoch 00007: loss improved from 0.74748 to 0.72765, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.7277 - acc: 0.8722\n",
            "Epoch 8/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.7092 - acc: 0.8742\n",
            "Epoch 00008: loss improved from 0.72765 to 0.70934, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "370/370 [==============================] - 10s 28ms/step - loss: 0.7093 - acc: 0.8742\n",
            "Epoch 9/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.8761\n",
            "Epoch 00009: loss improved from 0.70934 to 0.69189, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.6919 - acc: 0.8761\n",
            "Epoch 10/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.6751 - acc: 0.8784\n",
            "Epoch 00010: loss improved from 0.69189 to 0.67522, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.6752 - acc: 0.8784\n",
            "Q: 시험 공부 큰일\n",
            "A: 마음 이 좀 더 무너져요 \n",
            "\n",
            "\n",
            "Q: 아부 도 기술 인가 봐\n",
            "A: 마음 이 좀 더 무너져요 \n",
            "\n",
            "\n",
            "Q: 참아야 하나\n",
            "A: 마음 이 좀 더 무너져요 \n",
            "\n",
            "\n",
            "processing epoch: 21...\n",
            "Epoch 1/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.6591 - acc: 0.8804\n",
            "Epoch 00001: loss improved from 0.67522 to 0.65912, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.6591 - acc: 0.8804\n",
            "Epoch 2/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.6446 - acc: 0.8822\n",
            "Epoch 00002: loss improved from 0.65912 to 0.64455, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.6445 - acc: 0.8822\n",
            "Epoch 3/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.6307 - acc: 0.8839\n",
            "Epoch 00003: loss improved from 0.64455 to 0.63055, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.6306 - acc: 0.8840\n",
            "Epoch 4/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.6174 - acc: 0.8860\n",
            "Epoch 00004: loss improved from 0.63055 to 0.61766, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.6177 - acc: 0.8860\n",
            "Epoch 5/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.6053 - acc: 0.8875\n",
            "Epoch 00005: loss improved from 0.61766 to 0.60515, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.6052 - acc: 0.8875\n",
            "Epoch 6/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.5937 - acc: 0.8892\n",
            "Epoch 00006: loss improved from 0.60515 to 0.59356, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.5936 - acc: 0.8892\n",
            "Epoch 7/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.5820 - acc: 0.8911\n",
            "Epoch 00007: loss improved from 0.59356 to 0.58186, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.5819 - acc: 0.8912\n",
            "Epoch 8/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.5717 - acc: 0.8927\n",
            "Epoch 00008: loss improved from 0.58186 to 0.57181, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.5718 - acc: 0.8926\n",
            "Epoch 9/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.5615 - acc: 0.8943\n",
            "Epoch 00009: loss improved from 0.57181 to 0.56150, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.5615 - acc: 0.8943\n",
            "Epoch 10/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.5523 - acc: 0.8959\n",
            "Epoch 00010: loss improved from 0.56150 to 0.55227, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.5523 - acc: 0.8959\n",
            "Q: 주말 이 행복해\n",
            "A: 저 도 요 \n",
            "\n",
            "\n",
            "Q: 재회 를 한다해 도\n",
            "A: 저 도 모르는 걸 수도 있어요 \n",
            "\n",
            "\n",
            "Q: 남자 들 은 여자 가 자기 좋아하는 거 알 게 되면 어떻게 해 싫지 않다면 받아줘\n",
            "A: 저 도 요 \n",
            "\n",
            "\n",
            "processing epoch: 31...\n",
            "Epoch 1/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.5426 - acc: 0.8976\n",
            "Epoch 00001: loss improved from 0.55227 to 0.54271, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.5427 - acc: 0.8976\n",
            "Epoch 2/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.5346 - acc: 0.8986\n",
            "Epoch 00002: loss improved from 0.54271 to 0.53466, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.5347 - acc: 0.8986\n",
            "Epoch 3/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.5264 - acc: 0.9000\n",
            "Epoch 00003: loss improved from 0.53466 to 0.52620, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.5262 - acc: 0.9000\n",
            "Epoch 4/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.5184 - acc: 0.9014\n",
            "Epoch 00004: loss improved from 0.52620 to 0.51850, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.5185 - acc: 0.9013\n",
            "Epoch 5/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.5102 - acc: 0.9031\n",
            "Epoch 00005: loss improved from 0.51850 to 0.51016, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.5102 - acc: 0.9031\n",
            "Epoch 6/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.5028 - acc: 0.9039\n",
            "Epoch 00006: loss improved from 0.51016 to 0.50292, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.5029 - acc: 0.9039\n",
            "Epoch 7/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.4965 - acc: 0.9052\n",
            "Epoch 00007: loss improved from 0.50292 to 0.49641, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4964 - acc: 0.9052\n",
            "Epoch 8/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.9065\n",
            "Epoch 00008: loss improved from 0.49641 to 0.48952, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4895 - acc: 0.9065\n",
            "Epoch 9/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.4830 - acc: 0.9076\n",
            "Epoch 00009: loss improved from 0.48952 to 0.48305, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4831 - acc: 0.9076\n",
            "Epoch 10/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.4772 - acc: 0.9088\n",
            "Epoch 00010: loss improved from 0.48305 to 0.47715, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4772 - acc: 0.9088\n",
            "Q: 보험료 설계 다시 해야 하나\n",
            "A: 저 도 모르는 게 당연해요 \n",
            "\n",
            "\n",
            "Q: 힘듭니다 오늘 또 무너졌어\n",
            "A: 네 요 \n",
            "\n",
            "\n",
            "Q: 주름 도 멋진 사람\n",
            "A: 그게 인생 이 죠 \n",
            "\n",
            "\n",
            "processing epoch: 41...\n",
            "Epoch 1/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.4708 - acc: 0.9099\n",
            "Epoch 00001: loss improved from 0.47715 to 0.47077, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4708 - acc: 0.9099\n",
            "Epoch 2/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.4650 - acc: 0.9112\n",
            "Epoch 00002: loss improved from 0.47077 to 0.46498, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4650 - acc: 0.9112\n",
            "Epoch 3/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.4596 - acc: 0.9118\n",
            "Epoch 00003: loss improved from 0.46498 to 0.45956, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4596 - acc: 0.9118\n",
            "Epoch 4/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.4541 - acc: 0.9128\n",
            "Epoch 00004: loss improved from 0.45956 to 0.45406, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4541 - acc: 0.9128\n",
            "Epoch 5/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "368/370 [============================>.] - ETA: 0s - loss: 0.4482 - acc: 0.9143\n",
            "Epoch 00005: loss improved from 0.45406 to 0.44844, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4484 - acc: 0.9142\n",
            "Epoch 6/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.4431 - acc: 0.9149\n",
            "Epoch 00006: loss improved from 0.44844 to 0.44313, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4431 - acc: 0.9149\n",
            "Epoch 7/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.4375 - acc: 0.9162\n",
            "Epoch 00007: loss improved from 0.44313 to 0.43753, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4375 - acc: 0.9162\n",
            "Epoch 8/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.4332 - acc: 0.9169\n",
            "Epoch 00008: loss improved from 0.43753 to 0.43321, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4332 - acc: 0.9169\n",
            "Epoch 9/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.4287 - acc: 0.9177\n",
            "Epoch 00009: loss improved from 0.43321 to 0.42867, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4287 - acc: 0.9177\n",
            "Epoch 10/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.4236 - acc: 0.9186\n",
            "Epoch 00010: loss improved from 0.42867 to 0.42360, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4236 - acc: 0.9187\n",
            "Q: 자꾸 짝녀 얼굴 생각나네\n",
            "A: 마음 의 준비 가 될 거 예요 \n",
            "\n",
            "\n",
            "Q: 이별 2일 째\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 뭘 잘 못 했다는 걸까\n",
            "A: 마음 이 따뜻할 것 같아요 \n",
            "\n",
            "\n",
            "processing epoch: 51...\n",
            "Epoch 1/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.4194 - acc: 0.9194\n",
            "Epoch 00001: loss improved from 0.42360 to 0.41948, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.4195 - acc: 0.9194\n",
            "Epoch 2/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.4151 - acc: 0.9201\n",
            "Epoch 00002: loss improved from 0.41948 to 0.41514, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.4151 - acc: 0.9201\n",
            "Epoch 3/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.4095 - acc: 0.9212\n",
            "Epoch 00003: loss improved from 0.41514 to 0.40957, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4096 - acc: 0.9212\n",
            "Epoch 4/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.4059 - acc: 0.9219\n",
            "Epoch 00004: loss improved from 0.40957 to 0.40586, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4059 - acc: 0.9219\n",
            "Epoch 5/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.4019 - acc: 0.9224\n",
            "Epoch 00005: loss improved from 0.40586 to 0.40190, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.4019 - acc: 0.9224\n",
            "Epoch 6/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.3978 - acc: 0.9233\n",
            "Epoch 00006: loss improved from 0.40190 to 0.39776, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3978 - acc: 0.9233\n",
            "Epoch 7/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.3933 - acc: 0.9242\n",
            "Epoch 00007: loss improved from 0.39776 to 0.39334, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3933 - acc: 0.9242\n",
            "Epoch 8/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3896 - acc: 0.9248\n",
            "Epoch 00008: loss improved from 0.39334 to 0.38977, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.3898 - acc: 0.9247\n",
            "Epoch 9/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.9256\n",
            "Epoch 00009: loss improved from 0.38977 to 0.38503, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3850 - acc: 0.9256\n",
            "Epoch 10/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.9264\n",
            "Epoch 00010: loss improved from 0.38503 to 0.38163, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3816 - acc: 0.9265\n",
            "Q: 내 가 기대 를 너무 많이 했나 봐\n",
            "A: 마음 의 준비 가 필요했을지도 몰라요 \n",
            "\n",
            "\n",
            "Q: 짝사랑 하던 사람 못 잊을 것 같 애\n",
            "A: 마음 의 준비 를 하세요 \n",
            "\n",
            "\n",
            "Q: 결혼식 때 하객 이 없을 까봐 걱정 돼\n",
            "A: 마음 의 준비 가 안 됐다고 말 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 61...\n",
            "Epoch 1/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3779 - acc: 0.9270\n",
            "Epoch 00001: loss improved from 0.38163 to 0.37783, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.3778 - acc: 0.9270\n",
            "Epoch 2/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.9275\n",
            "Epoch 00002: loss improved from 0.37783 to 0.37506, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.3751 - acc: 0.9275\n",
            "Epoch 3/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.9281\n",
            "Epoch 00003: loss improved from 0.37506 to 0.37076, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3708 - acc: 0.9281\n",
            "Epoch 4/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.3662 - acc: 0.9292\n",
            "Epoch 00004: loss improved from 0.37076 to 0.36624, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3662 - acc: 0.9292\n",
            "Epoch 5/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.3628 - acc: 0.9296\n",
            "Epoch 00005: loss improved from 0.36624 to 0.36282, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3628 - acc: 0.9296\n",
            "Epoch 6/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.9303\n",
            "Epoch 00006: loss improved from 0.36282 to 0.35948, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3595 - acc: 0.9303\n",
            "Epoch 7/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3563 - acc: 0.9307\n",
            "Epoch 00007: loss improved from 0.35948 to 0.35643, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3564 - acc: 0.9306\n",
            "Epoch 8/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.3523 - acc: 0.9315\n",
            "Epoch 00008: loss improved from 0.35643 to 0.35234, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3523 - acc: 0.9315\n",
            "Epoch 9/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.9321\n",
            "Epoch 00009: loss improved from 0.35234 to 0.34919, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3492 - acc: 0.9321\n",
            "Epoch 10/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.3451 - acc: 0.9325\n",
            "Epoch 00010: loss improved from 0.34919 to 0.34514, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3451 - acc: 0.9325\n",
            "Q: 오늘 일도 안 했는데 엄청 피곤해\n",
            "A: 마음 이 약해 탈이네요 \n",
            "\n",
            "\n",
            "Q: 고백 했다 차이 면 어쩌지\n",
            "A: 마음 이 복잡하겠어요 \n",
            "\n",
            "\n",
            "Q: 예능 볼 게 없다\n",
            "A: 축하 드려요 \n",
            "\n",
            "\n",
            "processing epoch: 71...\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "370/370 [==============================] - ETA: 0s - loss: 0.3407 - acc: 0.9334\n",
            "Epoch 00001: loss improved from 0.34514 to 0.34066, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.3407 - acc: 0.9334\n",
            "Epoch 2/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.9340\n",
            "Epoch 00002: loss improved from 0.34066 to 0.33714, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.3371 - acc: 0.9339\n",
            "Epoch 3/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.9348\n",
            "Epoch 00003: loss improved from 0.33714 to 0.33353, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.3335 - acc: 0.9348\n",
            "Epoch 4/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.9351\n",
            "Epoch 00004: loss improved from 0.33353 to 0.32975, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3297 - acc: 0.9351\n",
            "Epoch 5/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.9356\n",
            "Epoch 00005: loss improved from 0.32975 to 0.32592, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3259 - acc: 0.9356\n",
            "Epoch 6/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.9361\n",
            "Epoch 00006: loss improved from 0.32592 to 0.32178, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3218 - acc: 0.9361\n",
            "Epoch 7/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.9369\n",
            "Epoch 00007: loss improved from 0.32178 to 0.31802, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3180 - acc: 0.9369\n",
            "Epoch 8/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.9374\n",
            "Epoch 00008: loss improved from 0.31802 to 0.31412, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3141 - acc: 0.9374\n",
            "Epoch 9/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.3103 - acc: 0.9379\n",
            "Epoch 00009: loss improved from 0.31412 to 0.31029, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3103 - acc: 0.9379\n",
            "Epoch 10/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.3067 - acc: 0.9388\n",
            "Epoch 00010: loss improved from 0.31029 to 0.30666, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.3067 - acc: 0.9388\n",
            "Q: 다시 한번 또\n",
            "A: 제 가 곁 에 있을게요 \n",
            "\n",
            "\n",
            "Q: 맨날 똑같 애\n",
            "A: 천천히 지워질 거 예요 \n",
            "\n",
            "\n",
            "Q: 여자친구 가 잘못 을 해도 다 공감 해주는게 옳은걸 까\n",
            "A: 잘 하고 있나 봐요 \n",
            "\n",
            "\n",
            "processing epoch: 81...\n",
            "Epoch 1/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.3028 - acc: 0.9391\n",
            "Epoch 00001: loss improved from 0.30666 to 0.30281, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.3028 - acc: 0.9391\n",
            "Epoch 2/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.2998 - acc: 0.9395\n",
            "Epoch 00002: loss improved from 0.30281 to 0.29984, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2998 - acc: 0.9395\n",
            "Epoch 3/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2961 - acc: 0.9402\n",
            "Epoch 00003: loss improved from 0.29984 to 0.29602, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2960 - acc: 0.9402\n",
            "Epoch 4/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.2928 - acc: 0.9407\n",
            "Epoch 00004: loss improved from 0.29602 to 0.29286, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2929 - acc: 0.9407\n",
            "Epoch 5/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.2891 - acc: 0.9413\n",
            "Epoch 00005: loss improved from 0.29286 to 0.28902, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.2890 - acc: 0.9413\n",
            "Epoch 6/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.2860 - acc: 0.9415\n",
            "Epoch 00006: loss improved from 0.28902 to 0.28597, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2860 - acc: 0.9415\n",
            "Epoch 7/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2822 - acc: 0.9424\n",
            "Epoch 00007: loss improved from 0.28597 to 0.28228, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2823 - acc: 0.9423\n",
            "Epoch 8/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2784 - acc: 0.9429\n",
            "Epoch 00008: loss improved from 0.28228 to 0.27837, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2784 - acc: 0.9429\n",
            "Epoch 9/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2741 - acc: 0.9434\n",
            "Epoch 00009: loss improved from 0.27837 to 0.27412, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2741 - acc: 0.9434\n",
            "Epoch 10/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.2710 - acc: 0.9439\n",
            "Epoch 00010: loss improved from 0.27412 to 0.27095, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2710 - acc: 0.9439\n",
            "Q: 화장품 이 필요해\n",
            "A: 그 사람 도 설렐 거 예요 \n",
            "\n",
            "\n",
            "Q: 나 한테 상의 좀 하지\n",
            "A: 좋은 걸 로 시작 해보세요 \n",
            "\n",
            "\n",
            "Q: 연애 세포 깨우는 법\n",
            "A: 사랑 은 끝나도 당신 의 인생 을 평가 할 수 없어요 \n",
            "\n",
            "\n",
            "processing epoch: 91...\n",
            "Epoch 1/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.2678 - acc: 0.9442\n",
            "Epoch 00001: loss improved from 0.27095 to 0.26782, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.2678 - acc: 0.9442\n",
            "Epoch 2/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.2643 - acc: 0.9450\n",
            "Epoch 00002: loss improved from 0.26782 to 0.26438, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.2644 - acc: 0.9450\n",
            "Epoch 3/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2610 - acc: 0.9452\n",
            "Epoch 00003: loss improved from 0.26438 to 0.26100, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.2610 - acc: 0.9452\n",
            "Epoch 4/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9461\n",
            "Epoch 00004: loss improved from 0.26100 to 0.25733, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2573 - acc: 0.9461\n",
            "Epoch 5/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.2545 - acc: 0.9460\n",
            "Epoch 00005: loss improved from 0.25733 to 0.25446, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2545 - acc: 0.9460\n",
            "Epoch 6/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.2513 - acc: 0.9467\n",
            "Epoch 00006: loss improved from 0.25446 to 0.25127, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2513 - acc: 0.9467\n",
            "Epoch 7/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2478 - acc: 0.9473\n",
            "Epoch 00007: loss improved from 0.25127 to 0.24786, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2479 - acc: 0.9473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.2453 - acc: 0.9478\n",
            "Epoch 00008: loss improved from 0.24786 to 0.24514, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2451 - acc: 0.9479\n",
            "Epoch 9/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2409 - acc: 0.9484\n",
            "Epoch 00009: loss improved from 0.24514 to 0.24096, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2410 - acc: 0.9484\n",
            "Epoch 10/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.2382 - acc: 0.9487\n",
            "Epoch 00010: loss improved from 0.24096 to 0.23816, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2382 - acc: 0.9487\n",
            "Q: 짝남 잊으려고 나 혼자 나쁘게 생각 하는 내 자신 이 증오 스러워\n",
            "A: 사랑 의 예의 가 없는 사람 이네 요 \n",
            "\n",
            "\n",
            "Q: 난 또 바보 ㅠㅠ\n",
            "A: 확신 이 들 때 까지 준비 해보세요 \n",
            "\n",
            "\n",
            "Q: 난 진짜 쓰레기 야\n",
            "A: 네 말씀 해주세요 \n",
            "\n",
            "\n",
            "processing epoch: 101...\n",
            "Epoch 1/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2363 - acc: 0.9488\n",
            "Epoch 00001: loss improved from 0.23816 to 0.23633, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.2363 - acc: 0.9488\n",
            "Epoch 2/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.2329 - acc: 0.9496\n",
            "Epoch 00002: loss improved from 0.23633 to 0.23294, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2329 - acc: 0.9496\n",
            "Epoch 3/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.9499\n",
            "Epoch 00003: loss improved from 0.23294 to 0.22996, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.2300 - acc: 0.9499\n",
            "Epoch 4/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.2260 - acc: 0.9508\n",
            "Epoch 00004: loss improved from 0.22996 to 0.22621, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2262 - acc: 0.9507\n",
            "Epoch 5/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.2229 - acc: 0.9512\n",
            "Epoch 00005: loss improved from 0.22621 to 0.22294, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2229 - acc: 0.9512\n",
            "Epoch 6/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2201 - acc: 0.9518\n",
            "Epoch 00006: loss improved from 0.22294 to 0.22017, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2202 - acc: 0.9517\n",
            "Epoch 7/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.2174 - acc: 0.9522\n",
            "Epoch 00007: loss improved from 0.22017 to 0.21743, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2174 - acc: 0.9522\n",
            "Epoch 8/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.2147 - acc: 0.9525\n",
            "Epoch 00008: loss improved from 0.21743 to 0.21461, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2146 - acc: 0.9525\n",
            "Epoch 9/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2121 - acc: 0.9530\n",
            "Epoch 00009: loss improved from 0.21461 to 0.21210, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2121 - acc: 0.9530\n",
            "Epoch 10/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.2092 - acc: 0.9533\n",
            "Epoch 00010: loss improved from 0.21210 to 0.20912, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2091 - acc: 0.9533\n",
            "Q: 이별 한지 한 달 된 남자 입니다\n",
            "A: 생각 을 정리 하는 방법 이 더 자주 만나세요 \n",
            "\n",
            "\n",
            "Q: 비밀 로 했는데 들켜서 오해 하고 있어\n",
            "A: 감기 조심하세요 \n",
            "\n",
            "\n",
            "Q: 으 휴 집 에 들어가기가 싫네\n",
            "A: 집 에 있거나 나가면 마스크 쓰고 나가세요 \n",
            "\n",
            "\n",
            "processing epoch: 111...\n",
            "Epoch 1/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.2066 - acc: 0.9537\n",
            "Epoch 00001: loss improved from 0.20912 to 0.20658, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2066 - acc: 0.9537\n",
            "Epoch 2/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9545\n",
            "Epoch 00002: loss improved from 0.20658 to 0.20348, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.2035 - acc: 0.9545\n",
            "Epoch 3/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9548\n",
            "Epoch 00003: loss improved from 0.20348 to 0.20091, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.2009 - acc: 0.9548\n",
            "Epoch 4/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1969 - acc: 0.9561\n",
            "Epoch 00004: loss improved from 0.20091 to 0.19690, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1969 - acc: 0.9561\n",
            "Epoch 5/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9560\n",
            "Epoch 00005: loss improved from 0.19690 to 0.19452, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1945 - acc: 0.9560\n",
            "Epoch 6/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1926 - acc: 0.9564\n",
            "Epoch 00006: loss improved from 0.19452 to 0.19260, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1926 - acc: 0.9564\n",
            "Epoch 7/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1895 - acc: 0.9568\n",
            "Epoch 00007: loss improved from 0.19260 to 0.18954, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1895 - acc: 0.9568\n",
            "Epoch 8/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9570\n",
            "Epoch 00008: loss improved from 0.18954 to 0.18752, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1875 - acc: 0.9570\n",
            "Epoch 9/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1851 - acc: 0.9575\n",
            "Epoch 00009: loss improved from 0.18752 to 0.18502, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1850 - acc: 0.9575\n",
            "Epoch 10/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1826 - acc: 0.9581\n",
            "Epoch 00010: loss improved from 0.18502 to 0.18261, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1826 - acc: 0.9581\n",
            "Q: 나 사랑 할 자격 없는 사람 이야\n",
            "A: 사랑 에 빠졌나 봐요 \n",
            "\n",
            "\n",
            "Q: 여자친구 가 너무 무뚝뚝해\n",
            "A: 그건 중요하지 않아요 신경 을 덜어 보세요 \n",
            "\n",
            "\n",
            "Q: 코골 이 어떻게 고쳐\n",
            "A: 피곤한건 아닌지 살펴보세요 \n",
            "\n",
            "\n",
            "processing epoch: 121...\n",
            "Epoch 1/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1834 - acc: 0.9578\n",
            "Epoch 00001: loss did not improve from 0.18261\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.1834 - acc: 0.9578\n",
            "Epoch 2/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9594\n",
            "Epoch 00002: loss improved from 0.18261 to 0.17730, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.1773 - acc: 0.9594\n",
            "Epoch 3/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1741 - acc: 0.9600\n",
            "Epoch 00003: loss improved from 0.17730 to 0.17409, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1741 - acc: 0.9600\n",
            "Epoch 4/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "369/370 [============================>.] - ETA: 0s - loss: 0.1721 - acc: 0.9602\n",
            "Epoch 00004: loss improved from 0.17409 to 0.17215, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1722 - acc: 0.9602\n",
            "Epoch 5/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.9608\n",
            "Epoch 00005: loss improved from 0.17215 to 0.16987, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1699 - acc: 0.9608\n",
            "Epoch 6/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.9609\n",
            "Epoch 00006: loss improved from 0.16987 to 0.16768, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1677 - acc: 0.9609\n",
            "Epoch 7/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1653 - acc: 0.9615\n",
            "Epoch 00007: loss improved from 0.16768 to 0.16532, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1653 - acc: 0.9615\n",
            "Epoch 8/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1643 - acc: 0.9618\n",
            "Epoch 00008: loss improved from 0.16532 to 0.16435, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1643 - acc: 0.9618\n",
            "Epoch 9/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1620 - acc: 0.9619\n",
            "Epoch 00009: loss improved from 0.16435 to 0.16197, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1620 - acc: 0.9619\n",
            "Epoch 10/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.9630\n",
            "Epoch 00010: loss improved from 0.16197 to 0.15817, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1582 - acc: 0.9630\n",
            "Q: 여자친구 와 여행 가는게 부담스러 움\n",
            "A: 내일 도 만난다면 말 을 걸어 보세요 \n",
            "\n",
            "\n",
            "Q: 힘들어\n",
            "A: 생각 보다 많은 시간 이 지났네요 \n",
            "\n",
            "\n",
            "Q: 요즘 드라마 너무 재밌네\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "processing epoch: 131...\n",
            "Epoch 1/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.9635\n",
            "Epoch 00001: loss improved from 0.15817 to 0.15641, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.1564 - acc: 0.9635\n",
            "Epoch 2/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1538 - acc: 0.9641\n",
            "Epoch 00002: loss improved from 0.15641 to 0.15379, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1538 - acc: 0.9641\n",
            "Epoch 3/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1521 - acc: 0.9645\n",
            "Epoch 00003: loss improved from 0.15379 to 0.15216, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1522 - acc: 0.9645\n",
            "Epoch 4/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9646\n",
            "Epoch 00004: loss improved from 0.15216 to 0.15020, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1502 - acc: 0.9646\n",
            "Epoch 5/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1475 - acc: 0.9649\n",
            "Epoch 00005: loss improved from 0.15020 to 0.14745, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1475 - acc: 0.9649\n",
            "Epoch 6/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.9648\n",
            "Epoch 00006: loss did not improve from 0.14745\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1492 - acc: 0.9648\n",
            "Epoch 7/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1448 - acc: 0.9657\n",
            "Epoch 00007: loss improved from 0.14745 to 0.14482, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1448 - acc: 0.9657\n",
            "Epoch 8/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1414 - acc: 0.9666\n",
            "Epoch 00008: loss improved from 0.14482 to 0.14138, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1414 - acc: 0.9666\n",
            "Epoch 9/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9669\n",
            "Epoch 00009: loss improved from 0.14138 to 0.13867, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1387 - acc: 0.9669\n",
            "Epoch 10/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.9670\n",
            "Epoch 00010: loss improved from 0.13867 to 0.13802, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.1380 - acc: 0.9670\n",
            "Q: 이러고 있을 때 가 아니야\n",
            "A: 행동 할 때 인 것 같네요 응원 해요 \n",
            "\n",
            "\n",
            "Q: 엿같다\n",
            "A: 벗어나는 게 좋겠네요 \n",
            "\n",
            "\n",
            "Q: 칭찬 좀 해봐\n",
            "A: 지금 도 늦지 않았어요 \n",
            "\n",
            "\n",
            "processing epoch: 141...\n",
            "Epoch 1/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1355 - acc: 0.9677\n",
            "Epoch 00001: loss improved from 0.13802 to 0.13554, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.1355 - acc: 0.9677\n",
            "Epoch 2/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1335 - acc: 0.9684\n",
            "Epoch 00002: loss improved from 0.13554 to 0.13353, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1335 - acc: 0.9684\n",
            "Epoch 3/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9683\n",
            "Epoch 00003: loss improved from 0.13353 to 0.13253, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 27ms/step - loss: 0.1325 - acc: 0.9683\n",
            "Epoch 4/10\n",
            "369/370 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9689\n",
            "Epoch 00004: loss improved from 0.13253 to 0.12975, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1298 - acc: 0.9689\n",
            "Epoch 5/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1289 - acc: 0.9690\n",
            "Epoch 00005: loss improved from 0.12975 to 0.12883, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1288 - acc: 0.9690\n",
            "Epoch 6/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1264 - acc: 0.9697\n",
            "Epoch 00006: loss improved from 0.12883 to 0.12654, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1265 - acc: 0.9696\n",
            "Epoch 7/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1251 - acc: 0.9698\n",
            "Epoch 00007: loss improved from 0.12654 to 0.12509, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1251 - acc: 0.9698\n",
            "Epoch 8/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9700\n",
            "Epoch 00008: loss improved from 0.12509 to 0.12327, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1233 - acc: 0.9700\n",
            "Epoch 9/10\n",
            "370/370 [==============================] - ETA: 0s - loss: 0.1219 - acc: 0.9705\n",
            "Epoch 00009: loss improved from 0.12327 to 0.12190, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1219 - acc: 0.9705\n",
            "Epoch 10/10\n",
            "368/370 [============================>.] - ETA: 0s - loss: 0.1193 - acc: 0.9712\n",
            "Epoch 00010: loss improved from 0.12190 to 0.11929, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "370/370 [==============================] - 10s 28ms/step - loss: 0.1193 - acc: 0.9712\n",
            "Q: 벌써 저 를 세번 째 떠나갔네\n",
            "A: 이 젠 잊어버리세요 미련 은 독 이 됩니다 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Q: 짝사랑 하는 사람과 친해질 수 있는 방법 조언 좀\n",
            "A: 공통 관심사 를 찾아보세요 \n",
            "\n",
            "\n",
            "Q: 짝남 한테 고백 한 다 안 한다\n",
            "A: 무시 당하는 기분 이 들어서 너무 외 롭고 상처 받게 된다고 차분하고 부드럽게 말 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 151...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dENxB9VmdxD7"
      },
      "source": [
        "## STEP 8. 예측 (prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibcTUOIj1_Mc"
      },
      "source": [
        "### 문제 23. `make_question` 함수 정의: 자연어 입력을 데이터 전처리 파이프라인 수행 후 전처리 진행하는 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuZ3q1NxdxD8"
      },
      "source": [
        "# 자연어 (질문 입력) 대한 전처리 함수\n",
        "def make_question(sentence):\n",
        "    # 코드를 입력하세요\n",
        "    sentence = clean_and_morph(sentence)\n",
        "    question_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "    return question_padded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlpEIeEd4ZcM"
      },
      "source": [
        "make_question('3박4일 놀러가고 싶다.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQKCJwtGdxD8"
      },
      "source": [
        "make_question('커피 마시고 싶다.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AijEFgfF2Mty"
      },
      "source": [
        "### 문제 24. `run_chatbot`함수를 정의합니다. 질문을 `make_question` 함수 전처리 수행후 모델 예측하여 결과를 다시 자연어로 변환하는 작업을 거치게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Q9NlZ8qdxD8"
      },
      "source": [
        "def run_chatbot(question):\n",
        "    # 코드를 입력하세요\n",
        "    question_inputs = make_question(question)\n",
        "    results = make_prediction(seq2seq, question_inputs)\n",
        "    results = convert_index_to_text(results, END_TOKEN)\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZREkyB-mdxD8"
      },
      "source": [
        "## STEP 9. 테스트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLcypsmo2eiI"
      },
      "source": [
        "### 문제 25. 사용자의 입력을 받아 대답을 출력해 주는 프로그램을 완성합니다.\r\n",
        "\r\n",
        "유저로부터 Text 입력 값을 받아 답변을 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAW5tIHNdxD8"
      },
      "source": [
        "# 코드를 입력하세요\n",
        "while True:\n",
        "    user_input = input('<< 말을 걸어 보세요!\\n')\n",
        "    if user_input == 'q':\n",
        "        break\n",
        "    print('>> 챗봇 응답: {}'.format(run_chatbot(user_input)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}